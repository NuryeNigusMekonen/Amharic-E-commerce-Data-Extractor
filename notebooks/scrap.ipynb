{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c412e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup import paths\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "# Import our modules\n",
    "import ingestion\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6853893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPE\n",
    "output_file = await ingestion.scrape()\n",
    "print(f\"Raw data saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1209c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Merged and cleaned dataset saved to: /home/nurye/Desktop/10_Academy/week_4/Amharic-E-commerce-Data-Extractor/data/raw/merged_cleaned.csv\n",
      " Final shape: (20779, 6)\n",
      " Columns: ['channel_name', 'message', 'timestamp', 'media_file', 'source_file', 'views']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Get notebook's current working directory\n",
    "base_dir = Path.cwd()\n",
    "\n",
    "# Step 2: Define full file paths\n",
    "raw_path1 = base_dir.parent / \"data/raw/scrapped_data_3.csv\"\n",
    "raw_path2 = base_dir.parent / \"data/raw/raw_20250619_150258.csv\"\n",
    "\n",
    "# Step 3: Load files as DataFrames\n",
    "df1 = pd.read_csv(raw_path1)\n",
    "df2 = pd.read_csv(raw_path2)\n",
    "\n",
    "# Step 4: Rename columns to match\n",
    "df1.rename(columns={\n",
    "    'Channel Title': 'channel_name',\n",
    "    'Message': 'message',\n",
    "    'Date': 'timestamp',\n",
    "    'Media Path': 'media_file'\n",
    "}, inplace=True)\n",
    "\n",
    "df2.rename(columns={\n",
    "    'channel': 'channel_name',\n",
    "    'message': 'message',\n",
    "    'timestamp': 'timestamp',\n",
    "    'media_file': 'media_file'\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 5: Drop unnecessary columns\n",
    "df1.drop(columns=['Channel Username', 'ID'], inplace=True, errors='ignore')\n",
    "df2.drop(columns=['sender_id'], inplace=True, errors='ignore')\n",
    "\n",
    "# Step 6: Tag source and fill in missing views for df1\n",
    "df1[\"source_file\"] = \"scrapped_data_3.csv\"\n",
    "df2[\"source_file\"] = \"raw_20250619_150258.csv\"\n",
    "\n",
    "np.random.seed(42)\n",
    "df1[\"views\"] = np.random.randint(100, 4000, size=len(df1))\n",
    "\n",
    "# Step 7: Merge\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True, sort=False)\n",
    "\n",
    "# Step 8: Clean\n",
    "merged_df['message'] = merged_df['message'].astype(str).str.strip()\n",
    "merged_df = merged_df[merged_df['message'].str.len() > 5]\n",
    "merged_df.drop_duplicates(subset=[\"message\", \"timestamp\"], inplace=True)\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 9: Save\n",
    "output_path = base_dir.parent / \"data/raw/merged_cleaned.csv\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\" Merged and cleaned dataset saved to:\", output_path)\n",
    "print(\" Final shape:\", merged_df.shape)\n",
    "print(\" Columns:\", merged_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "974b837a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved at: ../data/processed/processed_merged_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESS\n",
    "processed_file = preprocessing.preprocess('../data/raw/merged_cleaned.csv')\n",
    "\n",
    "print(f\"Processed file saved at: {processed_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb772266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20779 messages.\n"
     ]
    }
   ],
   "source": [
    "#lets make conll file\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "processed_file = '../data/processed/processed_merged_cleaned.csv'\n",
    "df = pd.read_csv(processed_file)\n",
    "\n",
    "print(f\"Loaded {len(df)} messages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfa6762f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CoNLL file saved to: ../data/labeled/ner_auto_labels.conll\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from sacremoses import MosesTokenizer\n",
    "tokenizer = MosesTokenizer(lang='am')\n",
    "def is_amharic_or_number(token):\n",
    "    \"\"\"Keep only Amharic script and digits\"\"\"\n",
    "    return bool(re.match(r'^[\\p{IsEthiopic}\\d፡።]+$', token))\n",
    "\n",
    "\n",
    "def auto_label(tokens):\n",
    "    labeled = []\n",
    "    prev_label = None\n",
    "    amharic_token_count = 0\n",
    "    price_mode = False\n",
    "    price_count = 0\n",
    "    loc_mode = False\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if not is_amharic_or_number(tok):\n",
    "            continue\n",
    "\n",
    "        # PRODUCT: First 2 Amharic tokens if first is not number\n",
    "        if len(labeled) == 0:\n",
    "            if re.fullmatch(r'\\d+', tok):\n",
    "                labeled.append((tok, 'O'))\n",
    "                continue\n",
    "            else:\n",
    "                labeled.append((tok, 'B-PRODUCT'))\n",
    "                amharic_token_count = 1\n",
    "                continue\n",
    "        elif amharic_token_count == 1:\n",
    "            labeled.append((tok, 'I-PRODUCT'))\n",
    "            amharic_token_count += 1\n",
    "            continue\n",
    "\n",
    "        # PRICE: Trigger on 'ዋጋ' \n",
    "        if tok == 'ዋጋ':\n",
    "            labeled.append((tok, 'B-PRICE'))\n",
    "            price_mode = True\n",
    "            price_count = 0\n",
    "            continue\n",
    "        elif price_mode:\n",
    "            price_count += 1\n",
    "            labeled.append((tok, 'I-PRICE'))\n",
    "\n",
    "            # Check if this is a price-ending token\n",
    "            if tok in {'ብር', 'ብ', 'ር'}:\n",
    "                price_mode = False\n",
    "            elif price_count >= 3:\n",
    "                price_mode = False\n",
    "            continue\n",
    "\n",
    "        # LOC: Trigger on 'አድራሻ' or 'አድራሻችን'\n",
    "        if tok in {'አድራሻ', 'አድራሻችን'}:\n",
    "            labeled.append((tok, 'B-LOC'))\n",
    "            loc_mode = True\n",
    "            continue\n",
    "        elif loc_mode:\n",
    "            labeled.append((tok, 'I-LOC'))\n",
    "            if tok in {'ፎቅ', 'ህንፃ'}:\n",
    "                loc_mode = False\n",
    "            continue\n",
    "\n",
    "        # DEFAULT \n",
    "        labeled.append((tok, 'O'))\n",
    "\n",
    "    return labeled\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_and_save_conll(input_csv, output_file, sample_size=None):\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    if sample_size:\n",
    "        df = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for message in df['clean_message']:\n",
    "            tokens = tokenizer.tokenize(str(message))\n",
    "            labeled_tokens = auto_label(tokens)\n",
    "            for tok, tag in labeled_tokens:\n",
    "                f.write(f\"{tok} {tag}\\n\")\n",
    "            f.write(\"\\n\")  # blank line between sentences\n",
    "\n",
    "    print(f\" CoNLL file saved to: {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_and_save_conll(\n",
    "        input_csv=\"../data/processed/processed_merged_cleaned.csv\",\n",
    "        output_file=\"../data/labeled/ner_auto_labels.conll\",\n",
    "        sample_size=100  \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91bbbd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ባለሁለት\tB-PRODUCT\n",
      "ምድጃ\tI-PRODUCT\n",
      "ስቶቭ\tO\n",
      "2000\tO\n",
      "ዋት\tO\n",
      "ፊውዝ\tO\n",
      "የተገጠመለት\tO\n",
      "ትልቅ\tO\n",
      "ድስት\tO\n",
      "መሸከም\tO\n",
      "የሚችል\tO\n",
      "አስተማማኝ\tO\n",
      "ቴርሞስታት\tO\n",
      "ባለ\tO\n",
      "ፊውዝ\tO\n",
      "ዋጋ\tB-PRICE\n",
      "፦\tI-PRICE\n",
      "ትልቁ\tI-PRICE\n",
      "2900ብር\tI-PRICE\n",
      "አድራሻ\tB-LOC\n",
      "መገናኛ\tI-LOC\n",
      "ስሪ\tI-LOC\n",
      "ኤም\tI-LOC\n",
      "ሲቲ\tI-LOC\n",
      "ሞል\tI-LOC\n",
      "ሁለተኛ\tI-LOC\n",
      "ፎቅ\tI-LOC\n",
      "ቢሮ\tO\n",
      "ቁ\tO\n",
      "ሊፍቱ\tO\n",
      "ፊት\tO\n",
      "ለ\tO\n",
      "ፊት\tO\n",
      "ለቡ\tO\n",
      "መዳህኒዓለም\tO\n",
      "ቤተክርስቲያን\tO\n",
      "ፊት\tO\n",
      "ለፊት\tO\n",
      "ዛም\tO\n",
      "ሞል\tO\n",
      "2ኛ\tO\n",
      "ፎቅ\tO\n",
      "ቢሮ\tO\n",
      "ለቡ\tO\n",
      "ቅርንጫፍ0973611819\tO\n",
      "0909522840\tO\n",
      "0923350054\tO\n",
      "ለማዘዝ\tO\n",
      "ይጠቀሙ\tO\n",
      "ለተጨማሪ\tO\n",
      "ማብራሪያ\tO\n",
      "የቴሌግራም\tO\n",
      "ገፃችን\tO\n"
     ]
    }
   ],
   "source": [
    "# Preview labeling on first row\n",
    "tokens = ast.literal_eval(df.iloc[0]['tokens'])\n",
    "labeled = auto_label(tokens)\n",
    "\n",
    "for tok, label in labeled:\n",
    "    print(f\"{tok}\\t{label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AE-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
