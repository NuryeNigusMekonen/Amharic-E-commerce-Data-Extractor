{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9465f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a208c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d2ac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: 'xlm-roberta-base'\n",
      "\n",
      "Label distribution:\n",
      "B-PRODUCT: 58\n",
      "I-PRODUCT: 58\n",
      "O: 3372\n",
      "B-LOC: 65\n",
      "I-LOC: 539\n",
      "B-PRICE: 30\n",
      "I-PRICE: 83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680dd0d3600b4e829f6c1e3822ff6f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nurye/Desktop/10_Academy/week_4/Amharic-E-commerce-Data-Extractor/AE-venv-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd050c98a0c476b92b114c2c15fbb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50617c02d9cf4b61a635a1388a3784d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592a3b67ccc5464082ee6fbba927db2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dde7a96dc714c79a5336525e5f7949e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 108.69105529785156, 'eval_precision': 0.12031691438398232, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.13062177029504918, 'eval_runtime': 0.3961, 'eval_samples_per_second': 50.49, 'eval_steps_per_second': 7.573, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5925ec29febd408a9c2c09302de46b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 68.08946228027344, 'eval_precision': 0.12031691438398232, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.13062177029504918, 'eval_runtime': 0.4047, 'eval_samples_per_second': 49.425, 'eval_steps_per_second': 7.414, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e0bb3258564a0899f61eb455e1bd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 40.08336639404297, 'eval_precision': 0.23486407231410958, 'eval_recall': 0.25735674648261714, 'eval_f1': 0.24538599430913707, 'eval_runtime': 0.3921, 'eval_samples_per_second': 51.009, 'eval_steps_per_second': 7.651, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5695c9a44dc34419a3eec1394c15dd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 29.309194564819336, 'eval_precision': 0.38211538926722205, 'eval_recall': 0.28363015845571543, 'eval_f1': 0.2649151782270495, 'eval_runtime': 0.3913, 'eval_samples_per_second': 51.114, 'eval_steps_per_second': 7.667, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3109ad13e5478dbb1e333b294ecaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 20.517202377319336, 'eval_precision': 0.7105284861407936, 'eval_recall': 0.6340527937795083, 'eval_f1': 0.6529743119884743, 'eval_runtime': 0.4168, 'eval_samples_per_second': 47.988, 'eval_steps_per_second': 7.198, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7089f93f2ee44534b3e4a4048008820b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 21.51284408569336, 'eval_precision': 0.6824375845593599, 'eval_recall': 0.6882887565474709, 'eval_f1': 0.6567478440445875, 'eval_runtime': 0.3752, 'eval_samples_per_second': 53.3, 'eval_steps_per_second': 7.995, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fb2ac286fb49199bac67686aae2594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 14.470846176147461, 'eval_precision': 0.7216333680056375, 'eval_recall': 0.7322139408712206, 'eval_f1': 0.7162434852029234, 'eval_runtime': 0.3813, 'eval_samples_per_second': 52.448, 'eval_steps_per_second': 7.867, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd41b050ebc41829dbccada24939c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 18.385986328125, 'eval_precision': 0.8877043873839853, 'eval_recall': 0.8889494099036792, 'eval_f1': 0.8851182677107102, 'eval_runtime': 0.372, 'eval_samples_per_second': 53.761, 'eval_steps_per_second': 8.064, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f5ea901ccf483f83ee68959cf78f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 14.285740852355957, 'eval_precision': 0.9083742730906744, 'eval_recall': 0.91414448027486, 'eval_f1': 0.9095455756513403, 'eval_runtime': 0.3835, 'eval_samples_per_second': 52.149, 'eval_steps_per_second': 7.822, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383400d6f08d4624a370b94ed3f38c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.455101013183594, 'eval_precision': 0.9208256898022361, 'eval_recall': 0.9312481602251184, 'eval_f1': 0.9251018670773499, 'eval_runtime': 0.3992, 'eval_samples_per_second': 50.098, 'eval_steps_per_second': 7.515, 'epoch': 10.0}\n",
      "{'train_runtime': 139.5858, 'train_samples_per_second': 5.66, 'train_steps_per_second': 1.433, 'train_loss': 39.18659423828125, 'epoch': 10.0}\n",
      "Finished training xlm-roberta-base. Saved to ../models/xlm-roberta-ner/checkpoint-200\n",
      "Loading model: 'rasyosef/bert-tiny-amharic'\n",
      "\n",
      "Label distribution:\n",
      "B-PRODUCT: 58\n",
      "I-PRODUCT: 58\n",
      "O: 3372\n",
      "B-LOC: 65\n",
      "I-LOC: 539\n",
      "B-PRICE: 30\n",
      "I-PRICE: 83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e928ad931e924b6eb8b40fcfeeff987d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nurye/Desktop/10_Academy/week_4/Amharic-E-commerce-Data-Extractor/AE-venv-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5898b54727545089475f78e469a1ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1b24fb54944c24b8600d3cbc18f325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at rasyosef/bert-tiny-amharic and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de20735f87b4982936f9fc7abe3b457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287cd2f478e94286a32e9c8286aebd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 135.71328735351562, 'eval_precision': 0.12449731052888868, 'eval_recall': 0.12201525746207381, 'eval_f1': 0.04593624470656171, 'eval_runtime': 0.1883, 'eval_samples_per_second': 106.241, 'eval_steps_per_second': 15.936, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66092473a5114ac6bbe1ab904ed20e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 101.205078125, 'eval_precision': 0.13774583483234495, 'eval_recall': 0.16121687190486889, 'eval_f1': 0.11227815652220892, 'eval_runtime': 0.1318, 'eval_samples_per_second': 151.724, 'eval_steps_per_second': 22.759, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f665bc50aca242bf9b5c84c59cecb992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 77.0671615600586, 'eval_precision': 0.17591118519628407, 'eval_recall': 0.14937331081527228, 'eval_f1': 0.12990213117423138, 'eval_runtime': 0.1359, 'eval_samples_per_second': 147.149, 'eval_steps_per_second': 22.072, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21af55d3dfa94d0d92b80eb9e0adae9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 62.11663818359375, 'eval_precision': 0.11597605352740932, 'eval_recall': 0.14213782189613006, 'eval_f1': 0.1277310924369748, 'eval_runtime': 0.136, 'eval_samples_per_second': 147.016, 'eval_steps_per_second': 22.052, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ecec2be499443fb0bd0319b8e10968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 53.4643440246582, 'eval_precision': 0.1157796451914099, 'eval_recall': 0.1427132786649403, 'eval_f1': 0.12784328887170565, 'eval_runtime': 0.1479, 'eval_samples_per_second': 135.256, 'eval_steps_per_second': 20.288, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28742b6f72e54c5495b71d2e418bb3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 48.604705810546875, 'eval_precision': 0.11580174927113702, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.12791446605693674, 'eval_runtime': 0.1407, 'eval_samples_per_second': 142.147, 'eval_steps_per_second': 21.322, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5561894928714d0cab877a24e295f21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 45.79034423828125, 'eval_precision': 0.11580174927113702, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.12791446605693674, 'eval_runtime': 0.1366, 'eval_samples_per_second': 146.373, 'eval_steps_per_second': 21.956, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469b573fdf014015b905aab384d42410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 43.85332489013672, 'eval_precision': 0.11580174927113702, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.12791446605693674, 'eval_runtime': 0.1407, 'eval_samples_per_second': 142.096, 'eval_steps_per_second': 21.314, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e90d17f50449b79d31e68df80f566b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 42.799049377441406, 'eval_precision': 0.11580174927113702, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.12791446605693674, 'eval_runtime': 0.146, 'eval_samples_per_second': 137.016, 'eval_steps_per_second': 20.552, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49489601cdd1490d90d7316903c91c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 42.32746505737305, 'eval_precision': 0.11580174927113702, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.12791446605693674, 'eval_runtime': 0.1391, 'eval_samples_per_second': 143.826, 'eval_steps_per_second': 21.574, 'epoch': 10.0}\n",
      "{'train_runtime': 30.449, 'train_samples_per_second': 25.945, 'train_steps_per_second': 6.568, 'train_loss': 89.02193359375, 'epoch': 10.0}\n",
      "Finished training rasyosef/bert-tiny-amharic. Saved to ../models/bert-tiny-amharic/checkpoint-200\n",
      "Loading model: 'Davlan/afro-xlmr-base'\n",
      "\n",
      "Label distribution:\n",
      "B-PRODUCT: 58\n",
      "I-PRODUCT: 58\n",
      "O: 3372\n",
      "B-LOC: 65\n",
      "I-LOC: 539\n",
      "B-PRICE: 30\n",
      "I-PRICE: 83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b118e042554e86879c33e7128e9b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nurye/Desktop/10_Academy/week_4/Amharic-E-commerce-Data-Extractor/AE-venv-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9c12e814f64a758debe1c4b3107a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675a13af565f4b19a729697bce2f8fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720bc8be6a2d454fa285fd245e9f5b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e38868fa714f63a2e121e2e89e6ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 74.1200942993164, 'eval_precision': 0.11197151757899422, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.1255426375929345, 'eval_runtime': 0.3528, 'eval_samples_per_second': 56.683, 'eval_steps_per_second': 8.502, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de311b9f5d74f66bbd37efc9da325cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 48.90926742553711, 'eval_precision': 0.11197151757899422, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.1255426375929345, 'eval_runtime': 0.3932, 'eval_samples_per_second': 50.865, 'eval_steps_per_second': 7.63, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4521efa773584b5ca8f6ef30a83b330b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 23.455154418945312, 'eval_precision': 0.25528769565466813, 'eval_recall': 0.27942656713542496, 'eval_f1': 0.2666266570783005, 'eval_runtime': 0.335, 'eval_samples_per_second': 59.694, 'eval_steps_per_second': 8.954, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e5301b4a284eb29cdda8c09ac766d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 17.587406158447266, 'eval_precision': 0.2622019289872365, 'eval_recall': 0.28389734272087214, 'eval_f1': 0.2725399228899719, 'eval_runtime': 0.3618, 'eval_samples_per_second': 55.273, 'eval_steps_per_second': 8.291, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3489372c8a094d4db2c7d60efbc744aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 15.722715377807617, 'eval_precision': 0.405466733788427, 'eval_recall': 0.3612635562738561, 'eval_f1': 0.37330947511413143, 'eval_runtime': 0.3225, 'eval_samples_per_second': 62.023, 'eval_steps_per_second': 9.304, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8684639b3d4109974ff18916ead890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.677871704101562, 'eval_precision': 0.7724122737228685, 'eval_recall': 0.6122632972531553, 'eval_f1': 0.6443447506003764, 'eval_runtime': 0.3221, 'eval_samples_per_second': 62.099, 'eval_steps_per_second': 9.315, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9db663ea2cd43119ad744c7ac9254f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.972696304321289, 'eval_precision': 0.7335669298156452, 'eval_recall': 0.7335849191832964, 'eval_f1': 0.7330135372174187, 'eval_runtime': 0.3523, 'eval_samples_per_second': 56.773, 'eval_steps_per_second': 8.516, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7436dc6cad4a5398a6e978631add56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.858081817626953, 'eval_precision': 0.7654879449250911, 'eval_recall': 0.782836937197992, 'eval_f1': 0.7697726383457483, 'eval_runtime': 0.3407, 'eval_samples_per_second': 58.707, 'eval_steps_per_second': 8.806, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b069aa20df5643949cf6c46f5f721525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.659294128417969, 'eval_precision': 0.7315917076330917, 'eval_recall': 0.7683747497134921, 'eval_f1': 0.7425172468085527, 'eval_runtime': 0.3713, 'eval_samples_per_second': 53.861, 'eval_steps_per_second': 8.079, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21abed7eba6f44c6b2d7a0f75d8cff12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.618353843688965, 'eval_precision': 0.7488755967985139, 'eval_recall': 0.7778985592373016, 'eval_f1': 0.755301008982495, 'eval_runtime': 0.3218, 'eval_samples_per_second': 62.148, 'eval_steps_per_second': 9.322, 'epoch': 10.0}\n",
      "{'train_runtime': 140.7546, 'train_samples_per_second': 5.613, 'train_steps_per_second': 1.421, 'train_loss': 39.1750244140625, 'epoch': 10.0}\n",
      "Finished training Davlan/afro-xlmr-base. Saved to ../models/afroxlmr-ner/checkpoint-200\n",
      "Loading model: 'google-bert/bert-base-multilingual-cased'\n",
      "\n",
      "Label distribution:\n",
      "B-PRODUCT: 58\n",
      "I-PRODUCT: 58\n",
      "O: 3372\n",
      "B-LOC: 65\n",
      "I-LOC: 539\n",
      "B-PRICE: 30\n",
      "I-PRICE: 83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e02808e3ed4166a6115916d1e0a60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nurye/Desktop/10_Academy/week_4/Amharic-E-commerce-Data-Extractor/AE-venv-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af58f5decd4b47a7b186d2d5c48c903a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b06631eb604a31b9288c1be6208377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f336c90c426f4a7faa2c2dce6f4077e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285a150ae6ca49138a7c60c625b8abc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 30.56558609008789, 'eval_precision': 0.11508398469981707, 'eval_recall': 0.14285714285714285, 'eval_f1': 0.1274753615179147, 'eval_runtime': 0.1863, 'eval_samples_per_second': 107.36, 'eval_steps_per_second': 16.104, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13663673ebf6418da240ab6fc1fb1b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 25.76980972290039, 'eval_precision': 0.2302089407191448, 'eval_recall': 0.21203904151881028, 'eval_f1': 0.21726424637518266, 'eval_runtime': 0.1872, 'eval_samples_per_second': 106.82, 'eval_steps_per_second': 16.023, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d9a75f3ad7488eac9b82aec7c986da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 22.149085998535156, 'eval_precision': 0.24962875573007462, 'eval_recall': 0.2115294485236682, 'eval_f1': 0.22162812994715603, 'eval_runtime': 0.1899, 'eval_samples_per_second': 105.324, 'eval_steps_per_second': 15.799, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb74fcfc33b4b81b925c64711d0507c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 16.26694107055664, 'eval_precision': 0.29574039603037117, 'eval_recall': 0.39303196673138874, 'eval_f1': 0.32996325900431145, 'eval_runtime': 0.2013, 'eval_samples_per_second': 99.368, 'eval_steps_per_second': 14.905, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df0d7ecf9c54c54811294031b247b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 14.77794075012207, 'eval_precision': 0.3121071761507404, 'eval_recall': 0.3997831440027972, 'eval_f1': 0.34373121965041264, 'eval_runtime': 0.1831, 'eval_samples_per_second': 109.227, 'eval_steps_per_second': 16.384, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f36cfd9ece467dafc30e013151475e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12.797861099243164, 'eval_precision': 0.387507678656747, 'eval_recall': 0.3984477871761109, 'eval_f1': 0.3905868982683854, 'eval_runtime': 0.2348, 'eval_samples_per_second': 85.185, 'eval_steps_per_second': 12.778, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0fdfbecbfa434e8b75ee9c208a25ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 14.636128425598145, 'eval_precision': 0.5197639430823288, 'eval_recall': 0.4586355840691101, 'eval_f1': 0.4069839864514952, 'eval_runtime': 0.1885, 'eval_samples_per_second': 106.091, 'eval_steps_per_second': 15.914, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab7d8b952254543bb8d2648f693d918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.8098726272583, 'eval_precision': 0.7522411569548224, 'eval_recall': 0.47535023430977186, 'eval_f1': 0.4526444605454682, 'eval_runtime': 0.1831, 'eval_samples_per_second': 109.239, 'eval_steps_per_second': 16.386, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b38365aa8f469f8f6f291a562e9216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.895940780639648, 'eval_precision': 0.7569989534275249, 'eval_recall': 0.49410142531529816, 'eval_f1': 0.4861420584150848, 'eval_runtime': 0.1877, 'eval_samples_per_second': 106.542, 'eval_steps_per_second': 15.981, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42511b9dfce4655af94873cf633b1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.097259521484375, 'eval_precision': 0.6348895357516048, 'eval_recall': 0.5333871396010125, 'eval_f1': 0.528353257131412, 'eval_runtime': 0.1886, 'eval_samples_per_second': 106.061, 'eval_steps_per_second': 15.909, 'epoch': 10.0}\n",
      "{'train_runtime': 80.6455, 'train_samples_per_second': 9.796, 'train_steps_per_second': 2.48, 'train_loss': 24.244951171875, 'epoch': 10.0}\n",
      "Finished training google-bert/bert-base-multilingual-cased. Saved to ../models/mbert-ner/checkpoint-200\n"
     ]
    }
   ],
   "source": [
    "from src.train import train_model\n",
    "from src.config import MODEL_LIST\n",
    "\n",
    "for model_name, model_dir in MODEL_LIST.items():\n",
    "    print(\"Loading model:\", repr(model_name))  #  Add this debug print\n",
    "    trainer, model, tokenizer = train_model(model_name, model_dir)\n",
    "    print(f\"Finished training {model_name}. Saved to {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26510fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " xlm-roberta: config.json missing. Generating from base model xlm-roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nurye/Desktop/10_Academy/week_4/Amharic-E-commerce-Data-Extractor/AE-venv-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " xlm-roberta: config.json generated.\n",
      " bert-tiny-amharic: config.json missing. Generating from base model rasyosef/bert-tiny-amharic...\n",
      " bert-tiny-amharic: config.json generated.\n",
      " afroxlmr: config.json missing. Generating from base model Davlan/afro-xlmr-base...\n",
      " afroxlmr: config.json generated.\n",
      " mbert: config.json missing. Generating from base model bert-base-multilingual-cased...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d4a246567044689a78fd239daf6386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mbert: config.json generated.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "import os\n",
    "\n",
    "# Define model directories and their base models\n",
    "MODELS = {\n",
    "    \"xlm-roberta\": {\n",
    "        \"path\": \"../models/xlm-roberta-ner/checkpoint-200\",\n",
    "        \"base\": \"xlm-roberta-base\"\n",
    "    },\n",
    "    \"bert-tiny-amharic\": {\n",
    "        \"path\": \"../models/bert-tiny-amharic/checkpoint-200\",\n",
    "        \"base\": \"rasyosef/bert-tiny-amharic\"\n",
    "    },\n",
    "    \"afroxlmr\": {\n",
    "        \"path\": \"../models/afroxlmr-ner/checkpoint-200\",\n",
    "        \"base\": \"Davlan/afro-xlmr-base\"\n",
    "    },\n",
    "    \"mbert\": {\n",
    "        \"path\": \"../models/mbert-ner/checkpoint-200\",\n",
    "        \"base\": \"bert-base-multilingual-cased\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def ensure_config(model_name, model_info):\n",
    "    config_path = os.path.join(model_info[\"path\"], \"config.json\")\n",
    "    if os.path.exists(config_path):\n",
    "        print(f\" {model_name}: config.json already exists.\")\n",
    "    else:\n",
    "        print(f\" {model_name}: config.json missing. Generating from base model {model_info['base']}...\")\n",
    "        config = AutoConfig.from_pretrained(model_info[\"base\"])\n",
    "        config.save_pretrained(model_info[\"path\"])\n",
    "        print(f\" {model_name}: config.json generated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for name, info in MODELS.items():\n",
    "        ensure_config(name, info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eca8f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing xlm-roberta-ner - loading tokenizer from xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nurye/Desktop/10_Academy/week_4/Amharic-E-commerce-Data-Extractor/AE-venv-py310/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer to ../models/xlm-roberta-ner/checkpoint-200\n",
      "\n",
      "Processing bert-tiny-amharic - loading tokenizer from rasyosef/bert-tiny-amharic\n",
      "Saved tokenizer to ../models/bert-tiny-amharic/checkpoint-200\n",
      "\n",
      "Processing afroxlmr-ner - loading tokenizer from Davlan/afro-xlmr-base\n",
      "Saved tokenizer to ../models/afroxlmr-ner/checkpoint-200\n",
      "\n",
      "Processing mbert-ner - loading tokenizer from bert-base-multilingual-cased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90757a958a564e2dafd9915a73545a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987844ebce464084acd198b67e010c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f497eeb92f2f48a69ffa51cbadb4ca14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer to ../models/mbert-ner/checkpoint-200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_BASES = {\n",
    "    \"xlm-roberta-ner\": \"xlm-roberta-base\",\n",
    "    \"bert-tiny-amharic\": \"rasyosef/bert-tiny-amharic\",\n",
    "    \"afroxlmr-ner\": \"Davlan/afro-xlmr-base\",\n",
    "    \"mbert-ner\": \"bert-base-multilingual-cased\",\n",
    "}\n",
    "\n",
    "for model_name, base_model_name in MODEL_BASES.items():\n",
    "    checkpoint_path = f\"../models/{model_name}/checkpoint-200\"  # adjust path if needed\n",
    "    print(f\"Processing {model_name} - loading tokenizer from {base_model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.save_pretrained(checkpoint_path)\n",
    "    print(f\"Saved tokenizer to {checkpoint_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c55f37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing: xlm-roberta-base\n",
      "ስልኩ        → B-PRODUCT\n",
      "0911223344 → O\n",
      "ነው         → O\n",
      "አድራሻችን     → B-LOC\n",
      "አዲስ        → I-LOC\n",
      "አበባ        → I-LOC\n",
      "ህንፃ        → I-LOC\n",
      "12         → I-LOC\n",
      "ዋጋ         → B-PRICE\n",
      "45         → I-PRICE\n",
      "ብር         → I-PRICE\n",
      "\n",
      " Testing: rasyosef/bert-tiny-amharic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at rasyosef/bert-tiny-amharic and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ስልኩ        → O\n",
      "0911223344 → O\n",
      "ነው         → O\n",
      "አድራሻችን     → B-LOC\n",
      "አዲስ        → O\n",
      "አበባ        → O\n",
      "ህንፃ        → O\n",
      "12         → O\n",
      "ዋጋ         → O\n",
      "45         → O\n",
      "ብር         → O\n",
      "\n",
      " Testing: Davlan/afro-xlmr-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ስልኩ        → B-PRODUCT\n",
      "0911223344 → I-PRODUCT\n",
      "ነው         → O\n",
      "አድራሻችን     → O\n",
      "አዲስ        → O\n",
      "አበባ        → O\n",
      "ህንፃ        → O\n",
      "12         → O\n",
      "ዋጋ         → O\n",
      "45         → O\n",
      "ብር         → O\n",
      "\n",
      " Testing: google-bert/bert-base-multilingual-cased\n",
      "ስልኩ        → O\n",
      "0911223344 → O\n",
      "ነው         → O\n",
      "አድራሻችን     → B-LOC\n",
      "አዲስ        → I-LOC\n",
      "አበባ        → I-LOC\n",
      "ህንፃ        → O\n",
      "12         → O\n",
      "ዋጋ         → O\n",
      "45         → O\n",
      "ብር         → O\n"
     ]
    }
   ],
   "source": [
    "from src.predict import load_model, predict_text\n",
    "from src.config import MODEL_LIST\n",
    "text = \"ስልኩ 0911223344 ነው አድራሻችን አዲስ አበባ ህንፃ 12 ዋጋ 45 ብር\"\n",
    "for model_name, path in MODEL_LIST.items():\n",
    "    print(f\"\\n Testing: {model_name}\")\n",
    "    model, tokenizer = load_model(model_name, path)\n",
    "    predictions = predict_text(text, model, tokenizer)\n",
    "    for word, tag in predictions:\n",
    "        print(f\"{word:<10} → {tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af62c6a",
   "metadata": {},
   "source": [
    "# #model comparion best model is xlm-roberta-base\n",
    "\n",
    "| Model Name                          | Eval F1   | Precision   | Recall    | Eval Loss   | Train Loss   | Train Time (s) | Samples/sec | Notes                  |\n",
    "| ----------------------------------- | --------- | ----------- | --------- | ----------- | ------------ | -------------- | ----------- | ---------------------- |\n",
    "| **1. xlm-roberta-base**             | **0.925** | **0.921**   | **0.931** | 13.46       | 39.19        | 139.59         | 5.66        |  Highest performance  |\n",
    "| **2. bert-tiny-amharic**            | 0.128     | 0.116       | 0.143     | 42.33       | 89.02        | 30.45          | **25.95**   |  Weak NER capability |\n",
    "| **3. afro-xlmr-base**               | 0.755     | 0.749       | 0.778     | **10.62**   | 39.18        | 140.75         | 5.61        |  Good balance         |\n",
    "| **4. bert-base-multilingual-cased** | 0.528     | 0.635       | 0.533     | 11.10       | 24.24        | 80.65          | 9.80        |  Moderate accuracy   |\n",
    "\n",
    "\n",
    "\n",
    "## why it is best\n",
    "Why it's best:\n",
    "Highest F1 score (0.925): Best balance of precision and recall.\n",
    "Strong generalization: Despite slightly higher eval loss than AfroXLM-R, F1 score is significantly better.\n",
    "Good training stability: Train loss is on par with afro-xlmr and lower than underperforming models.\n",
    "Well-suited for multilingual NER, especially with morphologically rich languages like Amharic.\"\"\"\n",
    "## Additional Observations:\n",
    "afro-xlmr-base is a solid second choice if looking for a model with slightly lower eval loss and smaller model size than XLM-R.\n",
    "bert-tiny-amharic performed poorly, likely due to small size and limited capacity—not recommended for NER.\n",
    "mBERT had moderate performance but lower F1 than AfroXLMR and XLM-R, making it a fallback option only.\"\"\"\n",
    "## Recommendation:\n",
    "Use xlm-roberta-base for deployment or further fine-tuning. It provides the best balance between accuracy and generalization for your Amharic NER task.\n",
    "If you want a slightly faster or smaller model for low-resource deployment, consider afro-xlmr-base as a good trade-off.\n",
    "Would you like a visualization or exportable table for documentation?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ea9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Processing: xlm-roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at ../models/xlm-roberta-ner/checkpoint-200 and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 LIME Interpretation for xlm-roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../models/bert-tiny-amharic/checkpoint-200 and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Processing: bert-tiny-amharic\n",
      "\n",
      "🔍 LIME Interpretation for bert-tiny-amharic\n",
      "\n",
      "🚀 Processing: afroxlmr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at ../models/afroxlmr-ner/checkpoint-200 and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 LIME Interpretation for afroxlmr\n",
      "\n",
      "🚀 Processing: mbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../models/mbert-ner/checkpoint-200 and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 LIME Interpretation for mbert\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set your models and labels\n",
    "MODEL_PATHS = {\n",
    "    \"xlm-roberta\": \"../models/xlm-roberta-ner/checkpoint-200/checkpoint-200\",\n",
    "    \"bert-tiny-amharic\": \"../models/bert-tiny-amharic/checkpoint-200/checkpoint-200\",\n",
    "    \"afroxlmr\": \"../models/afroxlmr-ner/checkpoint-200/checkpoint-200\",\n",
    "    \"mbert\": \"../models/mbert-ner/checkpoint-200/checkpoint-200\"\n",
    "}\n",
    "\n",
    "SAMPLE_TEXT = \"አዲስ አበባ ላይ ምርት ተመን 150 ብር ነበር።\"  # Amharic sentence\n",
    "\n",
    "def interpret_with_lime(model_name, pipeline_func):\n",
    "    print(f\"\\n🔍 LIME Interpretation for {model_name}\")\n",
    "    explainer = LimeTextExplainer(class_names=[\"O\", \"ENTITY\"])\n",
    "\n",
    "    def predict_proba(texts):\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            preds = pipeline_func(text)\n",
    "            # count how many words were tagged as entities\n",
    "            entity_count = sum(1 for p in preds if p.get(\"entity_group\", \"O\") != \"O\")\n",
    "            total_words = len(text.split())\n",
    "            avg = entity_count / total_words if total_words > 0 else 0\n",
    "            results.append([1 - avg, avg])\n",
    "        return np.array(results)\n",
    "\n",
    "    exp = explainer.explain_instance(SAMPLE_TEXT, predict_proba, num_features=10)\n",
    "    exp.save_to_file(f\"{model_name}_lime.html\")\n",
    "def process_model(model_name, model_path):\n",
    "    print(f\"\\n🚀 Processing: {model_name}\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "    # LIME\n",
    "    interpret_with_lime(model_name, ner_pipe)\n",
    "\n",
    "# Run for all models\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    process_model(name, path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AE-venv-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
